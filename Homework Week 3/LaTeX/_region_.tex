\message{ !name(mlex3.tex)}\documentclass[12pt,a4paper,twoside]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{hyperref}

\begin{document}

\message{ !name(mlex3.tex) !offset(-3) }

\section*{Machine Learning for Applications \\
in Computer Vision: Week 3}
\hrulefill

Liu, Nan \\
Wiedemann, Andreas\\

GitHub: \url{https://github.com/DJLoco/TUMMachineLearning.git}\\


\hrulefill

\subsection*{Exercise 1: Boosting}
The objective is to use the Boosting framework in MATLAB to train a AdaBoost classifier with the MNIST hand-written digits dataset. Here is our work:

\begin{itemize}
\item Load the MNIST data into MATLAB program, unpack the dataset and convert it into arrays: 
  \begin{itemize}
  \item Training images: 60000*784 uint8
  \item Training labels:  60000*1 uint8
  \item Testing images: 10000*784 uint8
  \item Testing labels: 10000*1 uint8
  \end{itemize}
\item Train the default AdaBoost on the first 1000 samples using fitensemble: 
ClassTreeEns = fitensemble(TrainImages(1:1000,:),TrainLabels(1:1000),'AdaBoostM2',1000,'Tree');
\item Test against the test set using misclassification error: 
L = loss(ClassTreeEns,TestImages,TestLabels); 

The classification error is 0.3765.
\item Plot the training error for 100, 200,… until 1000 the find the optimal number of learning rounds. According the figure, when learning rounds is 800, the resubstitution loss is the least,  which is 0.3.
\item Run the training on the entire dataset. The classification error becomes 0.3300.
\item Train the LPBoost on the first 1000 samples: 
ClassTreeEns = fitensemble(TrainImages(1:1000,:),TrainLabels(1:1000),’LPBoost’,1000,’Tree’);
The classification error is high, which is 0.6478. 
\end{itemize}

\subsection*{Exercise 2: Gaussian Process Classification}

\subsection*{Reference}

\end{document}
\message{ !name(mlex3.tex) !offset(-54) }
