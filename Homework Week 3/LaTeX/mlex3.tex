\documentclass[12pt,a4paper,twoside]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{hyperref}

\begin{document}

\section*{Machine Learning for Applications \\
in Computer Vision: Week 3}
\hrulefill

Liu, Nan \\
Wiedemann, Andreas\\

GitHub: \url{https://github.com/DJLoco/TUMMachineLearning.git}\\


\hrulefill

\subsection*{Exercise 1: Boosting}
The objective is to use the Boosting framework in MATLAB to train a AdaBoost classifier with the MNIST hand-written digits dataset. Here is our work:

\begin{itemize}
\item Load the MNIST data into MATLAB program, unpack the dataset and convert it into arrays: 
  \begin{itemize}
  \item Training images: 60000*784 uint8
  \item Training labels:  60000*1 uint8
  \item Testing images: 10000*784 uint8
  \item Testing labels: 10000*1 uint8
  \end{itemize}
\item Train the default AdaBoost on the first 1000 samples using fitensemble: 
\texttt{ClassTreeEns = fitensemble(TrainImages(1:1000,:),\\
\hspace*{35mm} TrainLabels(1:1000),'AdaBoostM2',1000,'Tree');}
\item Test against the test set using misclassification error:\\
\texttt{L = loss(ClassTreeEns,TestImages,TestLabels);}

The classification error is 0.3765.
\item According the figure, when learning rounds is 800, the resubstitution loss is the least,  which is 0.3.
\item Run the training on the entire dataset. The classification error becomes 0.3300.
\item Train the LPBoost on the first 1000 samples:\\
\texttt{ClassTreeEns = fitensemble(TrainImages(1:1000,:),\\
 \hspace*{35mm} TrainLabels(1:1000),’LPBoost’,1000,’Tree’);}

The classification error is high, which is 0.6478. 
\end{itemize}

\subsection*{Exercise 2: Gaussian Process Classification}
The objective is to get familiar with the Gaussian Process Classification tool and compare with the Boosting classification method. 

To get to know the Gaussian process we changed the parameters. The inference method specifies how the (approximate) posterior for a Gaussian process is calculated. Several methods are offered. We tried \texttt{'infEP'} and \texttt{'infLaplace'}, however there was no great impact on the result. 

For the covariance function we utilised \texttt{'covSEard'} and \texttt{'covSEiso'}.

Finally we tried different likelihood functions. There was a great change not only in the prediction but also the performance of the code. We obtained the most accurate results with \texttt{'likErf'} and \texttt{'likLogistic'}.

You can find our code \texttt{ex2.m} and \texttt{histo.m} within the doc folder of gpml.
Unfortunately the prediction of new test data seems to be incorrect. We tried to follow \texttt{demoClassification} by finding all the indices where $\exp(lp)>.5$. This is true for almost every test data. Thus, we can merely guess what the histogramms should show. False predictions will often happen when $p(x|X,y) \approx \frac 12$, which will lead to a maximal uncertainty.

\subsection*{Reference}

\url{http://de.mathworks.com/help/stats/fitensemble.html}

\url{http://yann.lecun.com/exdb/mnist/}

\url{http://gaussianprocess.org/}

\end{document}