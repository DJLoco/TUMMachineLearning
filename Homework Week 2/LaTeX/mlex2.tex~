\documentclass[12pt,a4paper,twoside]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}

\begin{document}
\section*{Machine Learning for Applications \\
in Computer Vision: Week 2}
\hrulefill

Backpropagation with single modified neuron:
\begin{itemize}
\item Input $x$: Set the corresponding activation $a_{j}$ for the input layer.
\item Feed forward: For each $l = 2,3,\cdots,L$ compute
\[z_{j}^{l} = f\left(\sum_{j} w_{j} x_{j}+b\right) \text{ and } a_{j}^{l} = \sigma(z_{j}^{l})\]
\item Output error $\delta_{j}^{L}$: ompute the vector
\[ \delta_{j}^{L} = \frac{\partial }{\partial a_{j}^{L}} \sigma(z_{j}^{L}) z_{j}^{l} (f_{j})\]
\item Back propagate the error: For each $l = L-1, L-2, \cdots , 2$ compute
  \[ \delta_{j}^{l} = ((w^{l+1})^{T}\delta^{l+1}) \odot \sigma'(z^{l})z^{l'}(f)\]
\item Output: The gradient of the cost function is given by
\[\frac{\partial }{\partial w_{jk}} = a_{k}^{l-1}\delta_{j}^{l} \text{ and }\frac{\partial}{\partial b_{j}^{l}}= \delta_{j}^{l}\]
\end{itemize}
\end{document}