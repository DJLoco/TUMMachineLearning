\documentclass[12pt,a4paper,twoside]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{hyperref}

\begin{document}
\section*{Machine Learning for Applications \\
in Computer Vision: Week 2}
\hrulefill

\subsection*{Exercice 2:}
Backpropagation with single modified neuron:
\begin{itemize}
\item Input $x$: Set the corresponding activation $a_{j}$ for the input layer.
\item Feed forward: For each $l = 2,3,\cdots,L$ compute
\[z_{j}^{l} = f\left(\sum_{j} w_{j} x_{j}+b\right) \text{ and } a_{j}^{l} = \sigma(z_{j}^{l})\]
\item Output error $\delta_{j}^{L}$: ompute the vector
\[ \delta_{j}^{L} = \frac{\partial }{\partial a_{j}^{L}} \sigma(z_{j}^{L}) z_{j}^{l} (f_{j})\]
\item Back propagate the error: For each $l = L-1, L-2, \cdots , 2$ compute
  \[ \delta_{j}^{l} = ((w^{l+1})^{T}\delta^{l+1}) \odot \sigma'(z^{l})z^{l'}(f)\]
\item Output: The gradient of the cost function is given by
\[\frac{\partial }{\partial w_{jk}} = a_{k}^{l-1}\delta_{j}^{l} \text{ and }\frac{\partial}{\partial b_{j}^{l}}= \delta_{j}^{l}\]
\end{itemize}

\subsection*{Exercice 3.2:}
The objective is to get to know the Caffe , a powerful framework of Neural Network and Deep Learning. First of all, to install, compile Caffe and run the example. Then use the Imagenet network  set up in the previous problem to classify each image downloaded from \url{https://vision.in.tum.de/teaching/ss2015/mlpractice_ss2015/slides}.

Before Classification, we preprocessed the images by setting the input channel order for BRG as needed for the reference ImageNet model, setting the mean to subtract for entering the data, letting the reference model operate on images in [0,255] instead of [0,1] and swapping the channel which makes the reference model has channel in BGR order instead of RGB.  

For the 8 images, we use the ImageNet network to predict the class of which, and provide the class name, the probability of that class, the entropy of the prediction. From the class names, we can see that the results are pretty accurate,  and the softmax  and entropy
\end{document}